<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2026-02-07T03:52:41+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">CHNN’s blog</title><subtitle>My blog about AI and technology</subtitle><entry><title type="html">Pyramid of Acceleration: A blueprint for interstellar humanity.</title><link href="http://localhost:4000/jekyll/update/2026/02/07/pyramid-of-acceleration.html" rel="alternate" type="text/html" title="Pyramid of Acceleration: A blueprint for interstellar humanity." /><published>2026-02-07T01:21:00+00:00</published><updated>2026-02-07T01:21:00+00:00</updated><id>http://localhost:4000/jekyll/update/2026/02/07/pyramid-of-acceleration</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2026/02/07/pyramid-of-acceleration.html"><![CDATA[<p>Humanity has long dreamed of transcending its limitations, harnessing the full energy of our star (a Kardashev Type II civilization) and reaching other stars. In this <strong>“Pyramid of Acceleration” </strong> model, the base layer is <strong> an artificial superintelligence</strong> (AI beyond human-level), the middle layer is <strong> quantum technology </strong> (quantum computing, sensing and communication), and the top layer is <strong> advanced spacecraft propulsion (high-speed starships)</strong>. Each layer supports the next: a superintelligent AI would drive rapid advances in quantum computing, and together they would enable the breakthroughs needed to build interstellar ships. Put another way, reaching Type II status (harvesting all our star’s available power) will require unprecedented leaps in intellect, computation, and propulsion.</p>
<ul>
  <li><strong>Artificial Superintelligence: </strong> AI systems surpassing human ability will turbo-charge research and design new technologies. For example, Lawrence Berkeley Lab notes that “Artificial Intelligence (AI) acts as a catalyst for accelerating scientific research and discovery”, using machine learning to optimize particle accelerators and other experiments. New AI models trained on physical laws can transfer insights across fields by applying knowledge from one domain to another. In practice, a superintelligent system might automatically sift through vast data, propose novel experiments, and design new materials or drugs, dramatically speeding every field of science.</li>
  <li><strong>Quantum Technology: </strong> Quantum computers and sensors will tackle problems far beyond classical machines. The <strong> NSF (National Science Foundation)</strong>  explains that quantum computers “could allow us to solve problems too complex for today’s most powerful computers,” from designing life-saving medicines to advancing new materials and secure communications. In essence, quantum machines use atomic-scale phenomena to simulate complex systems, for example, they could model entire molecules for drug discovery or optimize global supply chains. NVIDIA and national labs emphasize that AI and quantum computing are converging: as one blog states, <strong>“AI and quantum computing are no longer just distinct tools, but the foundational elements of a new class of supercomputers”</strong>. In this synergy, AI will help calibrate and control delicate quantum hardware and even discover new quantum algorithms, while quantum processors will in turn speed up AI by solving optimization problems faster.</li>
  <li><strong>Interstellar Propulsion:</strong> At the pyramid’s peak is advanced spacecraft capable of very high speeds (e.g. ≥50% the speed of light). Current rocket technology is orders of magnitude short of this. In fact, “no known technology can bring a rocket to relativistic speed,” notes a NASA summary  even a nuclear-pulse ship (Project Orion style) might only reach ~10% of light-speed. To hit 0.5c or more, we would need dramatic new physics. Theoretical concepts abound (antimatter engines, beamed light sails, or even warp drives), but all face staggering challenges. For example, constructing an Alcubierre-type warp drive would demand “10 times more negative energy than all of the positive energy in the universe”, effectively impossible with known physics. The Breakthrough Starshot proposal, an all-laser-driven lightsail  aimed for 15–20% of light speed for tiny probes. Achieving 50%+ the speed of light with a large, crewed ship would require exponentially more energy and near-perfect engineering. In short, the required propulsion and energy-generation technology does not yet exist.</li>
</ul>

<p><img src="/assets\css/images/poa.png" alt="Pyramid of Acceleration Diagram" /></p>

<p>Each of these layers is a <strong>massive challenge</strong> on its own. But together, they may form a pathway. A superintelligent AI could devise new quantum algorithms or physical insights that humans alone could not; quantum computers could then simulate the behavior of novel materials or fusion/plasma physics to guide the design of propulsion systems; and advanced AI-managed starships could use powerful quantum sensors to navigate the interstellar medium. In this way, the base of the pyramid (AI) and the middle (quantum tech) amplify one another and both underpin the possibility of the top (interstellar travel).</p>

<h2 id="artificial-superintelligence-asi--the-foundation">Artificial Superintelligence (ASI)- The Foundation</h2>

<p>The rapid evolution of Artificial Intelligence (AI) systems has been one of the defining technological trends of the past decade, demonstrating a truly unprecedented scale of advancement. This progression can be clearly charted by reviewing key milestones, beginning with systems like GPT-2 in 2019. This early iteration of a large language model, while groundbreaking for its time, pales in comparison to the capabilities demonstrated by cutting-edge AI just a few years later.</p>

<p>The journey from GPT-2, which showcased impressive natural language generation but still had significant limitations in complex reasoning and factual accuracy, to systems achieving world-class performance in specialized, high-level domains is staggering. A prime example of this accelerating trajectory is the creation of an AI capable of achieving a Gold Medalist score in the International Mathematics Olympiad (I.M.O.) in 2025.</p>

<p>This achievement represents a qualitative leap, moving AI from sophisticated pattern matching and text generation to genuine problem-solving, abstract reasoning, and mathematical creativity at an elite human level. The speed of this acceleration, a mere six years separating the foundational GPT-2 from an I.M.O. Gold Medalist AI, illustrates what could be termed a “Pyramid of Acceleration,” where each new generation of AI tools and computational power provides a foundation for dramatically faster and more complex development. This pace suggests a continued, exponential rise in AI capabilities across all fields, fundamentally reshaping the landscape of human and artificial intelligence.
Once we achieve <strong>Artificial General Intelligence (AGI)</strong>, an AI system with intellectual capability comparable to a human being, the initial acceleration will be merely a prelude. We will then “turn the crank” not just once, but likely two or three more times, entering a phase of recursive self-improvement that will quickly lead to AI systems becoming vastly superhuman, or <strong>“superintelligent.”</strong> This transition means the AI will become qualitatively smarter than the most brilliant human mind, much smarter, perhaps analogous to the cognitive gulf separating a fully developed adult from an elementary school-aged child, or even greater.
The sheer jump to superintelligence would be a seismic event even if AI progress continued at its current rapid-but-continuous, predictable rate. However, the timeline for this breakthrough could be dramatically condensed. <strong>In my opinion, there is a high probability to see such AI systems by 2031.</strong> The most potent accelerant is the potential for ASI to automate the very process of AI research itself. An ASI could design, execute, and analyze millions of experiments, develop novel algorithms, and discover fundamental architectural improvements in a time scale utterly impossible for human teams. This automation could trigger an <strong>intelligence explosion </strong>, where each new, slightly smarter generation of AI rapidly designs the next, culminating in an intelligence beyond human comprehension in a matter of days, weeks, or months, rather than years.</p>

<h2 id="quantum-technology--the-middle-layer">Quantum Technology – The Middle Layer</h2>

<p>Quantum technology represents a revolutionary paradigm shift in computation and sensing, encompassing not only <strong> quantum computers</strong> but also highly sensitive <strong>sensors</strong> and secure <strong>communication networks</strong>. The core of this revolution lies in quantum computers, which utilize quantum bits, or qubits. Unlike classical bits that can only represent a definite 0 or 1, qubits exploit the quantum phenomena of <strong>superposition</strong> and <strong> entanglement.</strong> Superposition allows a qubit to exist in a combination of both 0 and 1 simultaneously, dramatically increasing the amount of information it can hold. Entanglement links the fates of multiple qubits, meaning the state of one instantly influences the state of the others, regardless of the physical distance between them.
This inherent quantum parallelism allows quantum machines to explore an astronomical number of possibilities at once, making them exceptionally well-suited for tackling highly complex problems that render even the most powerful classical supercomputers ineffective. As highlighted by the National Science Foundation, these quantum machines <strong> “could allow us to solve problems too complex for today’s most powerful computers.”</strong> The potential applications are vast and transformative, promising breakthroughs across numerous fields.
For instance, in materials science, a quantum computer possesses the capability to simulate the precise quantum mechanical behavior of a novel material’s atoms and electrons, one by one. This level of fidelity is impossible to achieve with classical computation. Such simulations could lead to the discovery and design of entirely new substances, such as <strong>room-temperature superconductors, critical for a Type II civilization</strong> which would revolutionize energy transmission, or ultra-lightweight, high-strength alloys for aerospace applications. In the pharmaceutical industry, quantum computation could drastically accelerate the process of drug discovery by accurately modeling molecular interactions and protein folding, leading to the design of more effective and personalized medicines. Furthermore, the ability to optimize complex systems will profoundly impact logistics, finance, and artificial intelligence, solving problems like optimizing global shipping routes, portfolio risk analysis, and training more powerful machine learning models.</p>

<p><strong>Quantum sensors and networks</strong> are emerging as critical components in this new technological landscape. For example, the creation of a vast network of interconnected quantum computers and sensors holds the potential to share highly precise, real-world data. This data could include exceptionally accurate <strong>gravitational or magnetic field measurements</strong>, which, when combined, promise to significantly enhance navigation and observational capabilities, particularly for deep-space exploration and spacecraft guidance.
The convergence of advanced computational and sensing technologies is driving a profound transformation, often referred to as a “scientific revolution.” At the heart of this shift is the interplay between quantum systems, supercomputing, and artificial intelligence (AI).
The future of these technologies is inextricably linked, leading to a co-development trajectory. AI is expected to play a crucial role in enabling and refining quantum systems. Specifically, AI algorithms can be employed to help calibrate the exceedingly fragile and error-prone qubits: the fundamental building blocks of quantum computers and to discover more efficient and powerful quantum algorithms than humans could devise alone. Conversely, quantum machines are poised to accelerate AI by handling massive optimization tasks, such as training complex neural networks or processing vast datasets, at speeds unreachable by even the most advanced classical supercomputers.</p>

<h2 id="relativistic-spaceflight--the-top-layer">Relativistic Spaceflight – The Top Layer</h2>

<p>Achieving interstellar travel at a high fraction of light speed remains the ultimate aspiration in astronautics, facing immense physical and engineering obstacles. Reaching even 10–20% of the speed of light (c) is technologically challenging, and 50% or more is currently unconceived for massive, manned spacecraft.
The primary obstacle is energy.<strong> Special relativity dictates that accelerating mass (m) to relativistic speeds requires a tremendous kinetic energy </strong>. This immense energy dwarfs established propulsion capabilities; conventional chemical rockets are wholly inadequate. As one expert stated, <strong>“No known technology can bring a rocket to relativistic speed.”</strong>
Even advanced concepts like nuclear pulse rockets (e.g., Project Orion) would need “huge advances in propulsion, energy storage, and engine efficiency” to reach perhaps 10% of the speed of light, along with significant logistical and ethical hurdles regarding nuclear materials.
In contrast, Breakthrough Starshot proposes driving gram-scale “nanocraft” light sails to 15%–20% the speed of light using powerful ground-based lasers. While demonstrating potential for relativistic speed, these are tiny, unmanned probes. Scaling this laser-sail technology to accelerate multi-ton, crewed spacecraft for colonization or exploration remains speculative.</p>

<p>Several conceptual propulsion schemes have been proposed, though all are speculative:</p>
<ul>
  <li><strong>Laser Sails:</strong> Large ground or orbital lasers push a lightsail. Breakthrough Starshot envisioned sending tiny probes to Alpha Centauri (4.3 light years away) at ~20% of the speed of light, reaching the star in ~20–30 years. Scaling this to human-sized ships would require colossal energy and infrastructure (many gigawatts of laser power and kilometer-scale optics).</li>
  <li><strong>Nuclear Propulsion:</strong> Nuclear thermal rockets (fission/fusion engines) offer much higher specific impulse than chemical rockets. NASA is developing reactor-powered engines that could halve trip times to Mars, but even fusion drive concepts might only get to a few percent of the speed of light. The Wikipedia Relativistic Rocket entry stresses that known tech peaks around ~0.1c.</li>
  <li><strong>Antimatter Rockets:</strong> Matter–antimatter annihilation gives maximal energy per mass. In theory a small annihilation engine could approach relativistic speeds. Practically, we have no means to produce or store the kilograms of antimatter that would be needed.</li>
  <li><strong>Hypothetical Advanced Tech:</strong> Ideas like black hole funnels, beamed plasma, or quantum vacuum thrusters have been floated, but none are remotely proven.</li>
</ul>

<p>In short, our current propulsion concepts are far short of 0.5c. Even optimistic projections (e.g. von Neumann probes or generational starships) assume centuries or more to reach nearby stars. To shrink travel time to human lifetimes requires not just engineering tricks but new physics. Nevertheless, this top layer remains the ultimate target: a ship cruising at 50% the speed of light  would reach Proxima Centauri (the closest star to our solar system) in ~9 years, opening interstellar distances to exploration. Achieving that may demand the insights and materials that only a <strong>superintelligent AI and quantum-driven lab can create</strong>.</p>

<h2 id="the-path-to-a-kardashev-ii-civilization">The Path to a Kardashev II Civilization</h2>

<p>This three-tiered pyramid  AI, quantum, starships  is a conceptual roadmap. Each level amplifies the next. A superintelligent AI could revolutionize the design of quantum computers and starship components. Advanced quantum technology could make that AI even more powerful and help solve the physics of propulsion. Together, they could raise humanity to <strong>Type II </strong>, where we <strong> “access all the energy available of our star”</strong> and send probes or people to other stars.</p>

<p>Currently, Earth is still below Type I. But building the pyramid could change that trajectory. For instance, AI-driven research may find novel ways to harvest <strong> solar energy or create sustainable fusion</strong>. Quantum simulations might design ultra-efficient materials or batteries for spacecraft. And a cross-disciplinary AI could propose entirely new forms of propulsion we haven’t imagined. In this sense, the pyramid is visionary rather than predestined: it encapsulates how multiple breakthroughs could combine to accelerate progress.
In conclusion, the journey to a <strong>higher civilization status</strong> will not come from one “silver bullet.” It will come from the synergy of breakthroughs, the bricks of our pyramid. As labs and companies worldwide invest in AI, quantum computing, and space technology, each field propels the others forward. <strong>If we intentionally connect these advances, we may one day convert our planet’s vast energy into spectacular achievements: quantum-powered computers revolutionizing science, AI managing our complex systems, and starships exploring other suns. This is the grand vision of the acceleration pyramid,  a path from imagination to interstellar reality.</strong></p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Humanity has long dreamed of transcending its limitations, harnessing the full energy of our star (a Kardashev Type II civilization) and reaching other stars. In this “Pyramid of Acceleration” model, the base layer is an artificial superintelligence (AI beyond human-level), the middle layer is quantum technology (quantum computing, sensing and communication), and the top layer is advanced spacecraft propulsion (high-speed starships). Each layer supports the next: a superintelligent AI would drive rapid advances in quantum computing, and together they would enable the breakthroughs needed to build interstellar ships. Put another way, reaching Type II status (harvesting all our star’s available power) will require unprecedented leaps in intellect, computation, and propulsion. Artificial Superintelligence: AI systems surpassing human ability will turbo-charge research and design new technologies. For example, Lawrence Berkeley Lab notes that “Artificial Intelligence (AI) acts as a catalyst for accelerating scientific research and discovery”, using machine learning to optimize particle accelerators and other experiments. New AI models trained on physical laws can transfer insights across fields by applying knowledge from one domain to another. In practice, a superintelligent system might automatically sift through vast data, propose novel experiments, and design new materials or drugs, dramatically speeding every field of science. Quantum Technology: Quantum computers and sensors will tackle problems far beyond classical machines. The NSF (National Science Foundation) explains that quantum computers “could allow us to solve problems too complex for today’s most powerful computers,” from designing life-saving medicines to advancing new materials and secure communications. In essence, quantum machines use atomic-scale phenomena to simulate complex systems, for example, they could model entire molecules for drug discovery or optimize global supply chains. NVIDIA and national labs emphasize that AI and quantum computing are converging: as one blog states, “AI and quantum computing are no longer just distinct tools, but the foundational elements of a new class of supercomputers”. In this synergy, AI will help calibrate and control delicate quantum hardware and even discover new quantum algorithms, while quantum processors will in turn speed up AI by solving optimization problems faster. Interstellar Propulsion: At the pyramid’s peak is advanced spacecraft capable of very high speeds (e.g. ≥50% the speed of light). Current rocket technology is orders of magnitude short of this. In fact, “no known technology can bring a rocket to relativistic speed,” notes a NASA summary even a nuclear-pulse ship (Project Orion style) might only reach ~10% of light-speed. To hit 0.5c or more, we would need dramatic new physics. Theoretical concepts abound (antimatter engines, beamed light sails, or even warp drives), but all face staggering challenges. For example, constructing an Alcubierre-type warp drive would demand “10 times more negative energy than all of the positive energy in the universe”, effectively impossible with known physics. The Breakthrough Starshot proposal, an all-laser-driven lightsail aimed for 15–20% of light speed for tiny probes. Achieving 50%+ the speed of light with a large, crewed ship would require exponentially more energy and near-perfect engineering. In short, the required propulsion and energy-generation technology does not yet exist.]]></summary></entry><entry><title type="html">Group Sequence Policy Optimiztion (GSPO)</title><link href="http://localhost:4000/jekyll/update/2025/10/14/my-first-post.html" rel="alternate" type="text/html" title="Group Sequence Policy Optimiztion (GSPO)" /><published>2025-10-14T12:05:00+00:00</published><updated>2025-10-14T12:05:00+00:00</updated><id>http://localhost:4000/jekyll/update/2025/10/14/my-first-post</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2025/10/14/my-first-post.html"><![CDATA[<p>Over the past few years, Reinforcement Learning has become
the driving force behind the dramatic leaps in AI research.
From OpenAI’s RLHF to DeepSeek’s reasoning optimization and 
Qwen’s advanced training pipelines, the story of RL language
models is a story of stability, scalability and signal precision.</p>

<p>While this blog is about GSPO, we have to take a step back to
analyze its big bros: <strong>PPO</strong> and <strong>GRPO</strong>.</p>

<p>First, let’s talk about an important concept in RL: <strong>Importance Sampling</strong>.</p>

<h2 id="importance-sampling">Importance Sampling</h2>

<p>When training a Large Language Model with RL, we often face this problem: we want to estimate how good a 
new policy is, but our samples were generated by an old policy. So how do we make sure updates 
from the old samples still point in the right direction? That’s where Importance Sampling comes in.</p>

<p>Suppose you want to estimate the expected value of a function \(f(x)\) under a target distribution \(\pi_{\text{tar}}\),
but you can only sample from a different behaviour distribution \(\pi_{\text{beh}}\). Importance Sampling provides a way
to estimate the expected value by reweighting the samples from \(\pi_{\text{beh}}\).</p>
<div style="text-align: center;">
$$
\mathbb{E}_{z \sim \pi_{\text{tar}}}[f(z)] = \mathbb{E}_{z \sim \pi_{\text{beh}}} 
\left[ \frac{\pi_{\text{tar}}(z)}{\pi_{\text{beh}}(z)} f(z) \right]
$$
</div>

<p>That fraction \(\frac{\pi_{\text{tar}}(z)}{\pi_{\text{beh}}(z)}\) is called the importance weight. It acts like a 
translator, it adjusts old samples so they represent what the new policy would have done.
In RL, with Language Models:</p>
<ul>
  <li>The old policy is the model before the update \(\pi_{\theta_{\text{old}}}\)</li>
  <li>The new policy is the model we’re training \(\pi_{\theta}\)</li>
  <li>A sample \(z\) is a generated response \(y\) or sometimes a token \(y_{\text{t}}\).
So the importance weight becomes \(\frac{\pi_{\theta}(z)}{\pi_{\theta_{\text{old}}}(z)}\)</li>
</ul>

<p>That being said, let’s dive into our main card.</p>

<h2 id="ppo-proximal-policy-optimization">PPO: Proximal Policy Optimization</h2>

<p>In PPO, the model (the policy) generates responses, receives a scalar reward signal \(r\) from the reward model, and 
updates itself by comparing the probability of the new policy to that of the old one. 
At its core, PPO is built around a simple but profound idea: learn from past experiences but don’t stray too far 
from what worked before. When training with RL, we collect data: prompts and responses generated by the old policy 
\(\pi_{\theta_{\text{old}}}\) (think of it like an older version of the model). However, when we update the
model’s parameters to a new version \(\pi_{\theta}\), its probability distribution over responses changes.
This creates a mismatch: our training samples came from one policy but we’re now optimizing another one.</p>

<p>PPO addresses this mismatch by weighting each sample according to how much more likely it is under the new policy compared 
to the old one. For every generated token, PPO computes an importance ratio \(w_t(\theta) = \frac{\pi_\theta(y_t|x, y_{&lt;t})}{\pi_{\theta_{\text{old}}}(y_t|x, y_{&lt;t})}\)</p>

<p>This ratio acts as a “trust coefficient”.</p>
<ul>
  <li>If the new model \(\pi_\theta\) assigns a much higher probability \(w_{\text{t}} &gt;&gt; 1\) the sample
may be overemphasized, leading to unstable updates.</li>
  <li>If the new model \(\pi_\theta\) assigns a much lower probability \(w_{\text{t}} &lt;&lt; 1\) the sample
may be underemphasized, leading to suboptimal updates.</li>
</ul>

<p>This leads to PPO’s clipped objective:</p>
<div style="text-align: center;">
$$
\begin{equation}
J_{\text{PPO}}(\theta) = \mathbb{E}_{x \sim D, y \sim \pi_{\theta_{\text{old}}}(\cdot|x)} 
\left[ \frac{1}{|y|} \sum_{t=1}^{|y|} \min\left(w_t(\theta) \hat{A}_t, 
\text{clip}(w_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t\right) \right]
\end{equation}
$$
</div>

<ul>
  <li>\(\hat{A}_t\) is the estimated advantage of the \(t\)-th token, it defines how much better a token is than the average.
The advantage is computed via the <strong>Generalized Advantage Estimation (GAE)</strong> which involves a value model.</li>
  <li>\(\text{clip}(w_t(\theta), 1 - \epsilon, 1 + \epsilon)\) is the clipping term that ensures the updates are stable. clipping
makes PPO <strong>proximal</strong>: each update stays close to the previous one ensuring stable
learning while still allowing gradual improvement.</li>
</ul>

<p>Despite its elegance, PPO has a major limitation: it needs a value model as big as the policy to compute the advantage
introducing a <strong>considerable memory and computational burden</strong></p>

<h2 id="grpo-group-relative-policy-optimization">GRPO: Group Relative Policy Optimization</h2>

<p>Now that we have a good understanding of PPO, it’s super easy to understand GRPO.
GRPO keeps the spirit of PPO’s importance sampling but removes the value model entirely.</p>

<p>For each prompt \(x\), the model generates a group of \(G\) responses. Each response receives a scalar reward
from a verifier (reward model, external signal) \(r(x,y_i)\). Then, instead of using a learned value baseline, GRPO 
computes a relative advantage for each response.</p>
<div style="text-align: center;">
$$
\hat{A}_i = \frac{r(x, y_i) - \text{mean}\left(\{r(x, y_i)\}_{i=1}^G\right)}{\text{std}\left(\{r(x, y_i)\}_{i=1}^G\right)}
$$
</div>
<p>Once the relative advantages are computed, GRPO applies a PPO-like update for each token \(y_{\text{i,t}}\) in response \(y_i\).</p>
<div style="text-align: center;">
$$
w_{i,t}(\theta) = \frac{\pi_\theta(y_{i,t}|x, y_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(y_{i,t}|x, y_{i,&lt;t})}, \quad
$$
</div>

<p>The objective becomes:</p>
<div style="text-align: center;">
$$
\begin{equation}
J_{\text{GRPO}}(\theta) = \mathbb{E}_{x \sim D, \{y_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|x)} 
\left[ \frac{1}{G} \sum_{i=1}^{G} \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} 
\min\left(w_{i,t}(\theta) \hat{A}_{i,t}, \text{clip}\left(w_{i,t}(\theta), 1 - \epsilon, 1 + \epsilon\right) \hat{A}_{i,t}\right) \right],
\end{equation}
$$
</div>

<p>This makes GRPO computationally elegant, lightweight and surprisingly effective for tasks like mathematical reasoning, coding and instruction following.
The problem with GRPO is how its importance sampling is applied. The importance weights \(w_{i,t}\) are meant to correct the difference between the old and new policy. However, GRPO applies these corrections at the token level while the reward is given at the sequence level. This breaks the theoretical foundation of importance sampling. The results is <strong>high variance gradients</strong>.</p>

<h2 id="gspo-group-sequence-policy-optimization">GSPO: Group Sequence Policy Optimization</h2>

<p>GSPO and GRPO share the same workflow, the only and highly important difference is the way the importance sampling is applied. Compared to the previous algorithms, GSPO uses the <strong> geometric mean </strong>, thus redefining the importance ratio at <strong>sequence level</strong>.</p>
<div style="text-align: center;">
$$
\begin{equation}
s_i(\theta) = \left( \frac{\pi_\theta(y_i|x)}{\pi_{\theta_{\text{old}}}(y_i|x)} \right)^{\frac{1}{|y_i|}} 
= \exp\left( \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} \log \frac{\pi_\theta(y_{i,t}|x, y_{i,&lt;t})}{\pi_{\theta_{\text{old}}}(y_{i,t}|x, y_{i,&lt;t})} \right)
\end{equation}
$$
</div>

<p>Using this sequence level importance ratio, the objective becomes:</p>
<div style="text-align: center;">
$$
\begin{equation}
J_{\text{GSPO}}(\theta) = \mathbb{E}_{x \sim D, \{y_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|x)} 
\left[ \frac{1}{G} \sum_{i=1}^{G} \min\left(s_i(\theta) \hat{A}_i, \text{clip}\left(s_i(\theta), 1 - \epsilon, 1 + \epsilon\right) \hat{A}_i\right) \right],
\end{equation}
$$
</div>

<p>Just like in GRPO:</p>
<ul>
  <li>\(G\) is the number of responses generated for each prompt \(x\).</li>
  <li>\(\hat{A}_i\) is the normalized advantage of the \(i\)-th response.</li>
</ul>

<p>Every token in a response shares the same scaling factor \(s_i(\theta)\). This means the gradient contributions from all tokens move in a consistent direction. No more noisy tug-of-war between tokens in the same sentence.</p>

<p>The results from the tests made by the <strong>Qwen Team</strong> are impressive:</p>
<ul>
  <li><strong> Higher Sample Efficiency</strong>: It achieved better benchmark scores with the same number of queries.</li>
  <li><strong> MoE stability</strong>: Mixture-of-Experts models trained without any <strong>Routing Replay</strong>.</li>
</ul>

<p>Since the importance ratio is sequence based, it is unaffected by which experts activate for a given token. The MoE routing noise simply averages out at the sequence level.</p>

<p>Sometimes tasks demand finer granularity, for example in <strong> multi turn reasoning</strong> or <strong>step wise reward setups</strong>.
To address this, there is <strong> GSPO-token </strong>, a variant that keeps GSPO’s stable sequence level ratio but allows token specific advantages. It’s mathematically equivalent to GSPO when all token advantages are equal, but more flexible for tasks that need local credit assignment.</p>

<div style="text-align: center;">
$$
\begin{equation}
J_{\text{GSPO-token}}(\theta) = \mathbb{E}_{x \sim D, \{y_i\}_{i=1}^G \sim \pi_{\theta_{\text{old}}}(\cdot|x)} 
\left[ \frac{1}{G} \sum_{i=1}^{G} \frac{1}{|y_i|} \sum_{t=1}^{|y_i|} 
\min\left(s_{i,t}(\theta) \hat{A}_{i,t}, \text{clip}\left(s_{i,t}(\theta), 1 - \epsilon, 1 + \epsilon\right) \hat{A}_{i,t}\right) \right],
\end{equation}
$$
</div>

<p>where</p>
<div style="text-align: center;">
$$
\begin{equation}
s_{i,t}(\theta) = \text{sg}\left[s_i(\theta)\right] \cdot \frac{\pi_\theta(y_{i,t}|x, y_{i,&lt;t})}{\text{sg}\left[\pi_\theta(y_{i,t}|x, y_{i,&lt;t})\right]},
\end{equation}
$$
</div>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Over the past few years, Reinforcement Learning has become the driving force behind the dramatic leaps in AI research. From OpenAI’s RLHF to DeepSeek’s reasoning optimization and Qwen’s advanced training pipelines, the story of RL language models is a story of stability, scalability and signal precision.]]></summary></entry></feed>